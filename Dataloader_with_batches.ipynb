{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OugO9akhxSK0"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from scipy import sparse\n",
        "from skimage.util import img_as_float\n",
        "from scipy import io as scio\n",
        "from scipy import linalg\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BTL4Z986yBdM"
      },
      "outputs": [],
      "source": [
        "def detrend(input_signal, lambda_value):\n",
        "    signal_length = input_signal.shape[0]\n",
        "    # observation matrix\n",
        "    H = np.identity(signal_length)\n",
        "    ones = np.ones(signal_length)\n",
        "    minus_twos = -2 * np.ones(signal_length)\n",
        "    diags_data = np.array([ones, minus_twos, ones])\n",
        "    diags_index = np.array([0, 1, 2])\n",
        "    D = sparse.spdiags(diags_data, diags_index,\n",
        "                (signal_length - 2), signal_length).toarray()\n",
        "    filtered_signal = np.dot(\n",
        "        (H - np.linalg.inv(H + (lambda_value ** 2) * np.dot(D.T, D))), input_signal)\n",
        "    return filtered_signal\n",
        "\n",
        "\n",
        "# def process_video(frames):\n",
        "#     RGB = []\n",
        "#     for frame in frames:\n",
        "#         summation = np.sum(np.sum(frame, axis=0), axis=0)\n",
        "#         RGB.append(summation / (frame.shape[0] * frame.shape[1]))\n",
        "#     RGB = np.asarray(RGB)\n",
        "#     RGB = RGB.transpose(1, 0).reshape(1, 3, -1)\n",
        "#     return np.asarray(RGB)\n",
        "def process_video(frames):\n",
        "    RGB = []\n",
        "    for frame in frames:\n",
        "        summation = np.sum(np.sum(frame, axis=0), axis=0)\n",
        "        RGB.append(summation / (frame.shape[0] * frame.shape[1]))\n",
        "    RGB = np.asarray(RGB)\n",
        "\n",
        "    # Check if there's only one frame\n",
        "    if RGB.shape[0] == 1:\n",
        "        RGB = RGB.transpose(1, 0).reshape(3, -1)\n",
        "    else:\n",
        "        RGB = RGB.transpose(1, 0).reshape(1, 3, -1)\n",
        "\n",
        "    return np.asarray(RGB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B3lIQA8RyGdS"
      },
      "outputs": [],
      "source": [
        "\"\"\"ICA\n",
        "Non-contact, automated cardiac pulse measurements using video imaging and blind source separation.\n",
        "Poh, M. Z., McDuff, D. J., & Picard, R. W. (2010).\n",
        "Optics express, 18(10), 10762-10774. DOI: 10.1364/OE.18.010762\n",
        "\"\"\"\n",
        "\n",
        "def ICA_POH(frames, FS):\n",
        "    # Cut off frequency.\n",
        "    LPF = 0.7\n",
        "    HPF = 2.5\n",
        "    RGB = process_video(frames)\n",
        "\n",
        "    NyquistF = 1 / 2 * FS\n",
        "    BGRNorm = np.zeros(RGB.shape)\n",
        "    Lambda = 100\n",
        "    for c in range(3):\n",
        "        BGRDetrend = detrend(RGB[:, c], Lambda)\n",
        "        BGRNorm[:, c] = (BGRDetrend - np.mean(BGRDetrend)) / np.std(BGRDetrend)\n",
        "    _, S = ica(np.mat(BGRNorm).H, 3)\n",
        "\n",
        "    # select BVP Source\n",
        "    MaxPx = np.zeros((1, 3))\n",
        "    for c in range(3):\n",
        "        FF = np.fft.fft(S[c, :])\n",
        "        F = np.arange(0, FF.shape[1]) / FF.shape[1] * FS * 60\n",
        "        FF = FF[:, 1:]\n",
        "        FF = FF[0]\n",
        "        N = FF.shape[0]\n",
        "        Px = np.abs(FF[:math.floor(N / 2)])\n",
        "        Px = np.multiply(Px, Px)\n",
        "        Fx = np.arange(0, N / 2) / (N / 2) * NyquistF\n",
        "        Px = Px / np.sum(Px, axis=0)\n",
        "        MaxPx[0, c] = np.max(Px)\n",
        "    MaxComp = np.argmax(MaxPx)\n",
        "    BVP_I = S[MaxComp, :]\n",
        "    B, A = signal.butter(3, [LPF / NyquistF, HPF / NyquistF], 'bandpass')\n",
        "    BVP_F = signal.filtfilt(B, A, np.real(BVP_I).astype(np.double))\n",
        "\n",
        "    BVP = BVP_F[0]\n",
        "    return BVP\n",
        "\n",
        "\n",
        "def process_video(frames):\n",
        "    \"Calculates the average value of each frame.\"\n",
        "    RGB = []\n",
        "    for frame in frames:\n",
        "        sum = np.sum(np.sum(frame, axis=0), axis=0)\n",
        "        RGB.append(sum / (frame.shape[0] * frame.shape[1]))\n",
        "    return np.asarray(RGB)\n",
        "\n",
        "\n",
        "def ica(X, Nsources, Wprev=0):\n",
        "    nRows = X.shape[0]\n",
        "    nCols = X.shape[1]\n",
        "    if nRows > nCols:\n",
        "        print(\n",
        "            \"Warning - The number of rows is cannot be greater than the number of columns.\")\n",
        "        print(\"Please transpose input.\")\n",
        "\n",
        "    if Nsources > min(nRows, nCols):\n",
        "        Nsources = min(nRows, nCols)\n",
        "        print(\n",
        "            'Warning - The number of soures cannot exceed number of observation channels.')\n",
        "        print('The number of sources will be reduced to the number of observation channels ', Nsources)\n",
        "\n",
        "    Winv, Zhat = jade(X, Nsources, Wprev)\n",
        "    W = np.linalg.pinv(Winv)\n",
        "    return W, Zhat\n",
        "\n",
        "\n",
        "def jade(X, m, Wprev):\n",
        "    n = X.shape[0]\n",
        "    T = X.shape[1]\n",
        "    nem = m\n",
        "    seuil = 1 / math.sqrt(T) / 100\n",
        "    if m < n:\n",
        "        D, U = np.linalg.eig(np.matmul(X, np.mat(X).H) / T)\n",
        "        Diag = D\n",
        "        k = np.argsort(Diag)\n",
        "        pu = Diag[k]\n",
        "        ibl = np.sqrt(pu[n - m:n] - np.mean(pu[0:n - m]))\n",
        "        bl = np.true_divide(np.ones(m, 1), ibl)\n",
        "        W = np.matmul(np.diag(bl), np.transpose(U[0:n, k[n - m:n]]))\n",
        "        IW = np.matmul(U[0:n, k[n - m:n]], np.diag(ibl))\n",
        "    else:\n",
        "        IW = linalg.sqrtm(np.matmul(X, X.H) / T)\n",
        "        W = np.linalg.inv(IW)\n",
        "\n",
        "    Y = np.mat(np.matmul(W, X))\n",
        "    R = np.matmul(Y, Y.H) / T\n",
        "    C = np.matmul(Y, Y.T) / T\n",
        "    Q = np.zeros((m * m * m * m, 1))\n",
        "    index = 0\n",
        "\n",
        "    for lx in range(m):\n",
        "        Y1 = Y[lx, :]\n",
        "        for kx in range(m):\n",
        "            Yk1 = np.multiply(Y1, np.conj(Y[kx, :]))\n",
        "            for jx in range(m):\n",
        "                Yjk1 = np.multiply(Yk1, np.conj(Y[jx, :]))\n",
        "                for ix in range(m):\n",
        "                    Q[index] = np.matmul(Yjk1 / math.sqrt(T), Y[ix, :].T / math.sqrt(\n",
        "                        T)) - R[ix, jx] * R[lx, kx] - R[ix, kx] * R[lx, jx] - C[ix, lx] * np.conj(C[jx, kx])\n",
        "                    index += 1\n",
        "    # Compute and Reshape the significant Eigen\n",
        "    D, U = np.linalg.eig(Q.reshape(m * m, m * m))\n",
        "    Diag = abs(D)\n",
        "    K = np.argsort(Diag)\n",
        "    la = Diag[K]\n",
        "    M = np.zeros((m, nem * m), dtype=complex)\n",
        "    Z = np.zeros(m)\n",
        "    h = m * m - 1\n",
        "    for u in range(0, nem * m, m):\n",
        "        Z = U[:, K[h]].reshape((m, m))\n",
        "        M[:, u:u + m] = la[h] * Z\n",
        "        h = h - 1\n",
        "    # Approximate the Diagonalization of the Eigen Matrices:\n",
        "    B = np.array([[1, 0, 0], [0, 1, 1], [0, 0 - 1j, 0 + 1j]])\n",
        "    Bt = np.mat(B).H\n",
        "\n",
        "    encore = 1\n",
        "    if Wprev == 0:\n",
        "        V = np.eye(m).astype(complex)\n",
        "    else:\n",
        "        V = np.linalg.inv(Wprev)\n",
        "    # Main Loop:\n",
        "    while encore:\n",
        "        encore = 0\n",
        "        for p in range(m - 1):\n",
        "            for q in range(p + 1, m):\n",
        "                Ip = np.arange(p, nem * m, m)\n",
        "                Iq = np.arange(q, nem * m, m)\n",
        "                g = np.mat([M[p, Ip] - M[q, Iq], M[p, Iq], M[q, Ip]])\n",
        "                temp1 = np.matmul(g, g.H)\n",
        "                temp2 = np.matmul(B, temp1)\n",
        "                temp = np.matmul(temp2, Bt)\n",
        "                D, vcp = np.linalg.eig(np.real(temp))\n",
        "                K = np.argsort(D)\n",
        "                la = D[K]\n",
        "                angles = vcp[:, K[2]]\n",
        "                if angles[0, 0] < 0:\n",
        "                    angles = -angles\n",
        "                c = np.sqrt(0.5 + angles[0, 0] / 2)\n",
        "                s = 0.5 * (angles[1, 0] - 1j * angles[2, 0]) / c\n",
        "\n",
        "                if abs(s) > seuil:\n",
        "                    encore = 1\n",
        "                    pair = [p, q]\n",
        "                    G = np.mat([[c, -np.conj(s)], [s, c]])  # Givens Rotation\n",
        "                    V[:, pair] = np.matmul(V[:, pair], G)\n",
        "                    M[pair, :] = np.matmul(G.H, M[pair, :])\n",
        "                    temp1 = c * M[:, Ip] + s * M[:, Iq]\n",
        "                    temp2 = -np.conj(s) * M[:, Ip] + c * M[:, Iq]\n",
        "                    temp = np.concatenate((temp1, temp2), axis=1)\n",
        "                    M[:, Ip] = temp1\n",
        "                    M[:, Iq] = temp2\n",
        "\n",
        "    # Whiten the Matrix\n",
        "    # Estimation of the Mixing Matrix and Signal Separation\n",
        "    A = np.matmul(IW, V)\n",
        "    S = np.matmul(np.mat(V).H, Y)\n",
        "    return A, S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-QDbSJaUyQ_r"
      },
      "outputs": [],
      "source": [
        "def POS_WANG(frames, fs):\n",
        "    WinSec = 1.6\n",
        "    RGB = process_video(frames)\n",
        "    N = RGB.shape[0]\n",
        "    H = np.zeros((1, N))\n",
        "    l = math.ceil(WinSec * fs)\n",
        "\n",
        "    for n in range(N):\n",
        "        m = n - l\n",
        "        if m >= 0:\n",
        "            Cn = np.true_divide(RGB[m:n, :], np.mean(RGB[m:n, :], axis=0))\n",
        "            Cn = np.mat(Cn).H\n",
        "            S = np.matmul(np.array([[0, 1, -1], [-2, 1, 1]]), Cn)\n",
        "            h = S[0, :] + (np.std(S[0, :]) / np.std(S[1, :])) * S[1, :]\n",
        "            mean_h = np.mean(h)\n",
        "            for temp in range(h.shape[1]):\n",
        "                h[0, temp] = h[0, temp] - mean_h\n",
        "            H[0, m:n] = H[0, m:n] + (h[0])\n",
        "\n",
        "    BVP = H\n",
        "    BVP = detrend(np.mat(BVP).H, 100)\n",
        "    BVP = np.asarray(np.transpose(BVP))[0]\n",
        "    b, a = signal.butter(1, [0.75 / fs * 2, 3 / fs * 2], btype='bandpass')\n",
        "    BVP = signal.filtfilt(b, a, BVP.astype(np.double))\n",
        "    return BVP\n",
        "\n",
        "def CHROME_DEHAAN(frames,FS):\n",
        "    LPF = 0.7\n",
        "    HPF = 2.5\n",
        "    WinSec = 1.6\n",
        "\n",
        "    RGB = process_video(frames)\n",
        "    FN = RGB.shape[0]\n",
        "    NyquistF = 1/2*FS\n",
        "    B, A = signal.butter(3, [LPF/NyquistF, HPF/NyquistF], 'bandpass')\n",
        "\n",
        "    WinL = math.ceil(WinSec*FS)\n",
        "    if(WinL % 2):\n",
        "        WinL = WinL+1\n",
        "    NWin = math.floor((FN-WinL//2)/(WinL//2))\n",
        "    WinS = 0\n",
        "    WinM = int(WinS+WinL//2)\n",
        "    WinE = WinS+WinL\n",
        "    totallen = (WinL//2)*(NWin+1)\n",
        "    S = np.zeros(totallen)\n",
        "\n",
        "    for i in range(NWin):\n",
        "        RGBBase = np.mean(RGB[WinS:WinE, :], axis=0)\n",
        "        RGBNorm = np.zeros((WinE-WinS, 3))\n",
        "        for temp in range(WinS, WinE):\n",
        "            RGBNorm[temp-WinS] = np.true_divide(RGB[temp], RGBBase)\n",
        "        Xs = np.squeeze(3*RGBNorm[:, 0]-2*RGBNorm[:, 1])\n",
        "        Ys = np.squeeze(1.5*RGBNorm[:, 0]+RGBNorm[:, 1]-1.5*RGBNorm[:, 2])\n",
        "        Xf = signal.filtfilt(B, A, Xs, axis=0)\n",
        "        Yf = signal.filtfilt(B, A, Ys)\n",
        "\n",
        "        Alpha = np.std(Xf) / np.std(Yf)\n",
        "        SWin = Xf-Alpha*Yf\n",
        "        SWin = np.multiply(SWin, np.hanning(WinL))\n",
        "\n",
        "        temp = SWin[:int(WinL//2)]\n",
        "        S[WinS:WinM] = S[WinS:WinM] + SWin[:int(WinL//2)]\n",
        "        S[WinM:WinE] = SWin[int(WinL//2):]\n",
        "        WinS = WinM\n",
        "        WinM = WinS+WinL//2\n",
        "        WinE = WinS+WinL\n",
        "    BVP = S\n",
        "    return BVP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "42sISdjMyURq"
      },
      "outputs": [],
      "source": [
        "def _load_and_preprocess_video(video_path, batch_size=20):\n",
        "    path = 'haarcascade_frontalface_default.xml'\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + path)\n",
        "\n",
        "    face_frames = []\n",
        "    batch = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = video_path.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_frame = frame[y:y + h, x:x + w]\n",
        "            face_frame = cv2.resize(face_frame, (240, 240))\n",
        "            face_frame = face_frame / 255.0\n",
        "            batch.append(face_frame)\n",
        "\n",
        "            if len(batch) == batch_size:\n",
        "                face_frames.append(np.array(batch))\n",
        "                batch = []\n",
        "\n",
        "    if batch:  # If there are remaining frames in the last batch\n",
        "        face_frames.append(np.array(batch))\n",
        "\n",
        "    video_path.release()\n",
        "\n",
        "    return np.concatenate(face_frames, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vLfpbkgcyZdd"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# class HealthDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None):\n",
        "#         self.root_dir = root_dir\n",
        "#         self.samples = self._load_samples()\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def _load_samples(self):\n",
        "#         samples = []\n",
        "#         for dirpath, _, filenames in os.walk(self.root_dir):\n",
        "#             for filename in filenames:\n",
        "#                 if filename.endswith('.json'):\n",
        "#                     json_path = os.path.join(dirpath, filename)\n",
        "#                     with open(json_path) as json_file:\n",
        "#                         data = json.load(json_file)\n",
        "#                         for scenario in data['scenarios']:\n",
        "#                             scenario_settings = scenario['scenario_settings']\n",
        "#                             if (scenario_settings['position'] == \"Sitting\" and\n",
        "#                                     scenario_settings['facial_movement'] == \"No movement\" and\n",
        "#                                     scenario_settings['talking'] == \"N\"):\n",
        "#                                 rgb_filename = scenario['recordings']['RGB']['filename']\n",
        "#                                 if rgb_filename in filenames:\n",
        "#                                     sample = {\n",
        "#                                         \"GUID\": data['GUID'],\n",
        "#                                         \"participant_metadata\": {**data['participant'], 'GUID': data['GUID']},\n",
        "#                                         \"ppg_values\": [item[1] for item in scenario['recordings']['ppg']['timeseries']],\n",
        "#                                         \"recording_link\": os.path.join(dirpath, rgb_filename)\n",
        "#                                     }\n",
        "#                                     if 'bp_sys' in scenario['recordings'] and 'bp_dia' in scenario['recordings']:\n",
        "#                                         sample[\"bp_values\"] = {\n",
        "#                                             \"bp_sys\": scenario['recordings']['bp_sys']['value'],\n",
        "#                                             \"bp_dia\": scenario['recordings']['bp_dia']['value'],\n",
        "#                                         }\n",
        "#                                     samples.append(sample)\n",
        "#         return samples\n",
        "\n",
        "#     def process_and_save_ica_output(self, save_path='output.csv', batch_size=300):\n",
        "#           with open(save_path, 'w') as csv_file:\n",
        "#               csv_file.write(\"GUID,Gender,Age,PPG,ICA_Output,CHROME_Output,POS_Output,BP_Sys,BP_Dia\\n\")  # Header\n",
        "#               total_samples = len(self.samples)\n",
        "#               for idx, sample in enumerate(self.samples):\n",
        "#                   # Load and preprocess the video frames\n",
        "#                   cap = cv2.VideoCapture(sample['recording_link'])\n",
        "#                   num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "#                   ica_output_batches = []\n",
        "#                   chrome_output_batches = []\n",
        "#                   pos_output_batches = []\n",
        "\n",
        "#                   for i in range(0, num_frames, batch_size):\n",
        "#                       frames_batch = []\n",
        "#                       for _ in range(batch_size):\n",
        "#                           ret, frame = cap.read()\n",
        "#                           if not ret:\n",
        "#                               break\n",
        "#                           frames_batch.append(frame)\n",
        "#                       frames_batch = np.array(frames_batch)\n",
        "\n",
        "#                       # Pass facial frames through the ICA function\n",
        "#                       ica_output_batches.append(ICA_POH(frames_batch, FS=30))\n",
        "\n",
        "#                       # Pass facial frames through the CHROME_DEHAAN function\n",
        "#                       chrome_output_batches.append(CHROME_DEHAAN(frames_batch, FS=30))\n",
        "\n",
        "#                       # Pass facial frames through the POS_WANG function\n",
        "#                       pos_output_batches.append(POS_WANG(frames_batch, fs=30))\n",
        "\n",
        "#                   ica_output = np.concatenate(ica_output_batches)\n",
        "#                   chrome_output = np.concatenate(chrome_output_batches)\n",
        "#                   pos_output = np.concatenate(pos_output_batches)\n",
        "\n",
        "#                   # Convert the output arrays to string representations\n",
        "#                   ica_output_str = \",\".join(map(str, ica_output))\n",
        "#                   chrome_output_str = \",\".join(map(str, chrome_output))\n",
        "#                   pos_output_str = \",\".join(map(str, pos_output))\n",
        "\n",
        "#                   # Write GUID, Gender, Age, PPG, ICA output, CHROME output, POS output, BP Sys, and BP Dia to the CSV file\n",
        "#                   csv_file.write(f\"{sample['GUID']},{sample['participant_metadata']['gender']},{sample['participant_metadata']['age']},{','.join(map(str, sample['ppg_values']))},{ica_output_str},{chrome_output_str},{pos_output_str},{sample.get('bp_values', {}).get('bp_sys', '')},{sample.get('bp_values', {}).get('bp_dia', '')}\\n\")\n",
        "\n",
        "#                   progress_percentage = (idx + 1) / total_samples * 100\n",
        "#                   print(f\"\\rProcessing: {progress_percentage:.2f}% complete\", end='', flush=True)\n",
        "\n",
        "#               cap.release()\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         sample = self.samples[idx]\n",
        "\n",
        "#         # Load and preprocess the video frames\n",
        "#         video_frames = self._load_and_preprocess_video(cv2.VideoCapture(sample['recording_link']))\n",
        "\n",
        "#         # Convert data to PyTorch tensors\n",
        "#         ppg_values = torch.tensor(sample['ppg_values'], dtype=torch.float32)\n",
        "\n",
        "#         if 'bp_values' in sample:\n",
        "#             bp_values = torch.tensor([sample['bp_values']['bp_sys'], sample['bp_values']['bp_dia']], dtype=torch.float32)\n",
        "#             return {'ppg': ppg_values, 'video_frames': video_frames, 'bp': bp_values}\n",
        "\n",
        "#         return {'ppg': ppg_values, 'video_frames': video_frames, 'ica_output': torch.tensor(ICA_POH(video_frames, FS=30)),\n",
        "#                 'chrome_output': torch.tensor(CHROME_DEHAAN(video_frames, FS=30)),\n",
        "#                 'pos_output': torch.tensor(POS_WANG(video_frames, FS=30))}\n",
        "#     def __len__(self):\n",
        "#             return len(self.samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import signal\n",
        "\n",
        "class HealthDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.samples = self._load_samples()\n",
        "        self.transform = transform\n",
        "\n",
        "    def _load_samples(self):\n",
        "        samples = []\n",
        "        for dirpath, _, filenames in os.walk(self.root_dir):\n",
        "            for filename in filenames:\n",
        "                if filename.endswith('.json'):\n",
        "                    json_path = os.path.join(dirpath, filename)\n",
        "                    with open(json_path) as json_file:\n",
        "                        data = json.load(json_file)\n",
        "                        for scenario in data['scenarios']:\n",
        "                            scenario_settings = scenario['scenario_settings']\n",
        "                            if (scenario_settings['position'] == \"Sitting\" and\n",
        "                                    scenario_settings['facial_movement'] == \"No movement\" and\n",
        "                                    scenario_settings['talking'] == \"N\"):\n",
        "                                rgb_filename = scenario['recordings']['RGB']['filename']\n",
        "                                if rgb_filename in filenames:\n",
        "                                    sample = {\n",
        "                                        \"GUID\": data['GUID'],\n",
        "                                        \"participant_metadata\": {**data['participant'], 'GUID': data['GUID']},\n",
        "                                        \"ppg_values\": [item[1] for item in scenario['recordings']['ppg']['timeseries']],\n",
        "                                        \"recording_link\": os.path.join(dirpath, rgb_filename)\n",
        "                                    }\n",
        "                                    if 'bp_sys' in scenario['recordings'] and 'bp_dia' in scenario['recordings']:\n",
        "                                        sample[\"bp_values\"] = {\n",
        "                                            \"bp_sys\": scenario['recordings']['bp_sys']['value'],\n",
        "                                            \"bp_dia\": scenario['recordings']['bp_dia']['value'],\n",
        "                                        }\n",
        "                                    samples.append(sample)\n",
        "        return samples\n",
        "    def process_and_save_ica_output(self, save_path='output.csv', batch_size=300):\n",
        "        with open(save_path, 'w') as csv_file:\n",
        "            csv_file.write(\"GUID,Gender,Age,PPG,ICA_Output,CHROME_Output,POS_Output,BP_Sys,BP_Dia\\n\")  # Header\n",
        "            total_samples = len(self.samples)\n",
        "            for idx, sample in enumerate(self.samples):\n",
        "                # Load and preprocess the video frames\n",
        "                cap = cv2.VideoCapture(sample['recording_link'])\n",
        "                num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "                print(f\"Total number of frames: {num_frames}\")  # Print total number of frames\n",
        "\n",
        "                ica_output_batches = []\n",
        "                chrome_output_batches = []\n",
        "                pos_output_batches = []\n",
        "\n",
        "                for i in range(0, num_frames, batch_size):\n",
        "                    frames_batch = []\n",
        "                    for _ in range(batch_size):\n",
        "                        ret, frame = cap.read()\n",
        "                        if not ret:\n",
        "                            break\n",
        "                        frames_batch.append(frame)\n",
        "                    if frames_batch:  # Check if frames_batch is not empty\n",
        "                        frames_batch = np.array(frames_batch)\n",
        "\n",
        "                        # Pass facial frames through the ICA function\n",
        "                        ica_output_batches.append(ICA_POH(frames_batch, FS=30))\n",
        "\n",
        "                        # Pass facial frames through the CHROME_DEHAAN function\n",
        "                        chrome_output_batches.append(CHROME_DEHAAN(frames_batch, FS=30))\n",
        "\n",
        "                        # Pass facial frames through the POS_WANG function\n",
        "                        pos_output_batches.append(POS_WANG(frames_batch, fs=30))\n",
        "\n",
        "                if ica_output_batches:\n",
        "                    ica_output = np.concatenate(ica_output_batches)\n",
        "                else:\n",
        "                    ica_output = np.array([])\n",
        "\n",
        "                if chrome_output_batches:\n",
        "                    chrome_output = np.concatenate(chrome_output_batches)\n",
        "                else:\n",
        "                    chrome_output = np.array([])\n",
        "\n",
        "                if pos_output_batches:\n",
        "                    pos_output = np.concatenate(pos_output_batches)\n",
        "                else:\n",
        "                    pos_output = np.array([])\n",
        "\n",
        "                # Convert the output arrays to string representations\n",
        "                ica_output_str = ','.join(map(str, ica_output))\n",
        "                chrome_output_str = ','.join(map(str, chrome_output))\n",
        "                pos_output_str = ','.join(map(str, pos_output))\n",
        "\n",
        "                # Write GUID, Gender, Age, PPG, ICA output, CHROME output, POS output, BP Sys, and BP Dia to the CSV file\n",
        "                # csv_file.write(f\"{sample['GUID']},{sample['participant_metadata']['gender']},{sample['participant_metadata']['age']},{','.join(map(str, sample['ppg_values']))},{ica_output_str},{chrome_output_str},{pos_output_str},{sample.get('bp_values', {}).get('bp_sys', '')},{sample.get('bp_values', {}).get('bp_dia', '')}\\n\")\n",
        "                #csv_file.write(f\"{sample['GUID']},{sample['participant_metadata']['gender']},{sample['participant_metadata']['age']},{','.join(map(str, sample['ppg_values']))},{','.join(map(str, ica_output))},{','.join(map(str, chrome_output))},{','.join(map(str, pos_output))},{sample.get('bp_values', {}).get('bp_sys', '')},{sample.get('bp_values', {}).get('bp_dia', '')}\\n\")\n",
        "                # csv_file.write(f\"{sample['GUID']},{sample['participant_metadata']['gender']},{sample['participant_metadata']['age']},[{','.join(map(str, sample['ppg_values']))}],[{','.join(map(str, ica_output))}],[{','.join(map(str, chrome_output))}],[{','.join(map(str, pos_output))}],{sample.get('bp_values', {}).get('bp_sys', '')},{sample.get('bp_values', {}).get('bp_dia', '')}\\n\")\n",
        "                csv_file.write(f\"{sample['GUID']},{sample['participant_metadata']['gender']},{sample['participant_metadata']['age']},\\\"[{','.join(map(str, sample['ppg_values']))}]\\\",\\\"[{','.join(map(str, ica_output))}]\\\",\\\"[{','.join(map(str, chrome_output))}]\\\",\\\"[{','.join(map(str, pos_output))}]\\\",{sample.get('bp_values', {}).get('bp_sys', '')},{sample.get('bp_values', {}).get('bp_dia', '')}\\n\")\n",
        "\n",
        "                progress_percentage = (idx + 1) / total_samples * 100\n",
        "                print(f\"\\rProcessing: {progress_percentage:.2f}% complete\", end='', flush=True)\n",
        "\n",
        "                cap.release()\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Load and preprocess the video frames\n",
        "        video_frames = self._load_and_preprocess_video(cv2.VideoCapture(sample['recording_link']))\n",
        "\n",
        "        # Convert data to PyTorch tensors\n",
        "        ppg_values = torch.tensor(sample['ppg_values'], dtype=torch.float32)\n",
        "\n",
        "        if 'bp_values' in sample:\n",
        "            bp_values = torch.tensor([sample['bp_values']['bp_sys'], sample['bp_values']['bp_dia']], dtype=torch.float32)\n",
        "            return {'ppg': ppg_values, 'video_frames': video_frames, 'bp': bp_values}\n",
        "\n",
        "        return {'ppg': ppg_values, 'video_frames': video_frames, 'ica_output': torch.tensor(np.concatenate(ICA_POH(video_frames, FS=30)), dtype=torch.float32),\n",
        "                'chrome_output': torch.tensor(np.concatenate(CHROME_DEHAAN(video_frames, FS=30)), dtype=torch.float32),\n",
        "                'pos_output': torch.tensor(np.concatenate(POS_WANG(video_frames, FS=30)), dtype=torch.float32)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "# Your ICA_POH, CHROME_DEHAAN, and POS_WANG functions would be defined here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "85OLKWt-ycTb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of frames: 900\n",
            "Processing: 0.50% completeTotal number of frames: 900\n",
            "Processing: 1.00% completeTotal number of frames: 900\n",
            "Processing: 1.49% completeTotal number of frames: 900\n",
            "Processing: 1.99% completeTotal number of frames: 900\n",
            "Processing: 2.49% completeTotal number of frames: 900\n",
            "Processing: 2.99% completeTotal number of frames: 900\n",
            "Processing: 3.48% completeTotal number of frames: 900\n",
            "Processing: 3.98% completeTotal number of frames: 900\n",
            "Processing: 4.48% completeTotal number of frames: 900\n",
            "Processing: 4.98% completeTotal number of frames: 900\n",
            "Processing: 5.47% completeTotal number of frames: 900\n",
            "Processing: 5.97% completeTotal number of frames: 900\n",
            "Processing: 6.47% completeTotal number of frames: 0\n",
            "Processing: 6.97% completeTotal number of frames: 900\n",
            "Processing: 7.46% completeTotal number of frames: 900\n",
            "Processing: 7.96% completeTotal number of frames: 900\n",
            "Processing: 8.46% completeTotal number of frames: 900\n",
            "Processing: 8.96% completeTotal number of frames: 900\n",
            "Processing: 9.45% completeTotal number of frames: 0\n",
            "Processing: 9.95% completeTotal number of frames: 900\n",
            "Processing: 10.45% completeTotal number of frames: 900\n",
            "Processing: 10.95% completeTotal number of frames: 900\n",
            "Processing: 11.44% completeTotal number of frames: 900\n",
            "Processing: 11.94% completeTotal number of frames: 900\n",
            "Processing: 12.44% completeTotal number of frames: 900\n",
            "Processing: 12.94% completeTotal number of frames: 0\n",
            "Processing: 13.43% completeTotal number of frames: 900\n",
            "Processing: 13.93% completeTotal number of frames: 900\n",
            "Processing: 14.43% completeTotal number of frames: 900\n",
            "Processing: 14.93% completeTotal number of frames: 900\n",
            "Processing: 15.42% completeTotal number of frames: 900\n",
            "Processing: 15.92% completeTotal number of frames: 900\n",
            "Processing: 16.42% completeTotal number of frames: 900\n",
            "Processing: 16.92% completeTotal number of frames: 900\n",
            "Processing: 17.41% completeTotal number of frames: 0\n",
            "Processing: 17.91% completeTotal number of frames: 900\n",
            "Processing: 18.41% completeTotal number of frames: 900\n",
            "Processing: 18.91% completeTotal number of frames: 900\n",
            "Processing: 19.40% completeTotal number of frames: 900\n",
            "Processing: 19.90% completeTotal number of frames: 0\n",
            "Processing: 20.40% completeTotal number of frames: 900\n",
            "Processing: 20.90% completeTotal number of frames: 900\n",
            "Processing: 21.39% completeTotal number of frames: 900\n",
            "Processing: 21.89% completeTotal number of frames: 900\n",
            "Processing: 22.39% completeTotal number of frames: 900\n",
            "Processing: 22.89% completeTotal number of frames: 900\n",
            "Processing: 23.38% completeTotal number of frames: 900\n",
            "Processing: 23.88% completeTotal number of frames: 900\n",
            "Processing: 24.38% completeTotal number of frames: 900\n",
            "Processing: 24.88% completeTotal number of frames: 900\n",
            "Processing: 25.37% completeTotal number of frames: 900\n",
            "Processing: 25.87% completeTotal number of frames: 900\n",
            "Processing: 26.37% completeTotal number of frames: 900\n",
            "Processing: 26.87% completeTotal number of frames: 900\n",
            "Processing: 27.36% completeTotal number of frames: 900\n",
            "Processing: 27.86% completeTotal number of frames: 900\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/Dataset/VV/vv100\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m health_dataset \u001b[38;5;241m=\u001b[39m HealthDataset(root_dir)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mhealth_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_and_save_ica_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOUT1.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[7], line 60\u001b[0m, in \u001b[0;36mHealthDataset.process_and_save_ica_output\u001b[1;34m(self, save_path, batch_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m frames_batch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m---> 60\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "root_dir = 'D:/Dataset/VV/vv100'\n",
        "health_dataset = HealthDataset(root_dir)\n",
        "health_dataset.process_and_save_ica_output(save_path='OUT1.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
